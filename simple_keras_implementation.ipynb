{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gfc1OMd0WeGG",
    "outputId": "3a45878f-9887-4f0e-f9e4-d724cd1c745f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import Model as Model_\n",
    "from tensorflow.keras.layers import Input, ReLU, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
    "\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VJlcJTFQWsoI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "class Dataset_Preprocessing:\n",
    "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, total_classes = 17):\n",
    "        \n",
    "        #Dataset Directory path\n",
    "        self.dir_path = dir_path\n",
    "        \n",
    "        #Which Dimension file to include, possible values: 2 and 3\n",
    "        self.include_dimension = include_dimension\n",
    "        \n",
    "        #Total frames in one Sample\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        #Activity classes to include\n",
    "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
    "        \n",
    "        #Total activity classes\n",
    "        self.total_classes = len(self.classes)\n",
    "        \n",
    "        #Subject Folders names in the Dataset\n",
    "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
    "    \n",
    "    def read_dataset(self):\n",
    "        try:\n",
    "            #Contains all the different activity vectors\n",
    "            activity_vector = {}\n",
    "            \n",
    "            #Contains the overall dataset\n",
    "            sampled_data = None\n",
    "            \n",
    "            #Based on dimensions, which folder to use for extracting the dataset files\n",
    "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
    "            \n",
    "            #Checking if the dataset path is valid\n",
    "            if not os.path.exists(self.dir_path):\n",
    "                print('The Data Directory Does not Exist!')\n",
    "                return None\n",
    "\n",
    "            #Iterating over all the subject folders\n",
    "            for fld in self.internal_folders:\n",
    "                #Iterating for each file in the specified folder\n",
    "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
    "                    #Extracting the activity from the filename\n",
    "                    activity = self.__extract_activity(file)\n",
    "                    \n",
    "                    if activity not in self.classes:\n",
    "                        continue\n",
    "                    \n",
    "                    #Reading the CSV file using Pandas\n",
    "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
    "\n",
    "                    #Formulating the activity vector using one hot encoding\n",
    "                    if activity not in activity_vector:\n",
    "                        total_keys = len(activity_vector.keys())\n",
    "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
    "                        activity_vector[activity][total_keys] = 1\n",
    "                    vector = activity_vector[activity]\n",
    "                    \n",
    "                    #Sampling the dataset\n",
    "                    grouped_sample = self.__group_samples(data, self.sample_size, vector)\n",
    "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
    "                    \n",
    "            return sampled_data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __extract_activity(self, filename):\n",
    "        try:\n",
    "            #Extracting the filename and excluding the extension\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            \n",
    "            #Substituting the empty string with characters other than english alphabets\n",
    "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
    "            return activity\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __group_samples(self, dataset, sample_size, activity):\n",
    "        try:\n",
    "            #Checking if the dataset is a Pandas Dataframe\n",
    "            if not isinstance(dataset, pd.DataFrame):\n",
    "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
    "                return None\n",
    "            \n",
    "            #Appending activity class to each row in the dataset\n",
    "            dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
    "            \n",
    "            #Reshaping the dataset into sample batches\n",
    "            total_samples = dataset.shape[0]//sample_size\n",
    "            total_features = dataset.shape[1]\n",
    "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
    "            \n",
    "            return grouped_rows\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eud56W84W80a"
   },
   "outputs": [],
   "source": [
    "#For short term prediction, we need a sample size of 20(10 frames input sequance, 10 frames predicted sequance)\n",
    "sampled_data = Dataset_Preprocessing('./H3.6csv', sample_size=30).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmD7-mCDXAPI"
   },
   "outputs": [],
   "source": [
    "total_batches = sampled_data.shape[0]\n",
    "sampled_data = sampled_data[:total_batches-(total_batches%100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eed-DrIHXCU4"
   },
   "outputs": [],
   "source": [
    "def split_to_features_labels(dataset, input_sequance_size=10) :\n",
    "    assert input_sequance_size < dataset.shape[1], f\"input sequance should be smaller than the total sample size\"\n",
    "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
    "    labels = dataset[:,np.s_[input_sequance_size:], :64]\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7TbKYYsfXEsn"
   },
   "outputs": [],
   "source": [
    "sampled_dataX, sampled_dataY = split_to_features_labels(sampled_data, input_sequance_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MbSHOacXHjf",
    "outputId": "06d380ad-aef7-47a9-e041-9b2f887f6c5a"
   },
   "outputs": [],
   "source": [
    "print('Total Samples: {}'.format(sampled_dataY.shape[0]))\n",
    "print('Total Frames: {}'.format(sampled_dataY.shape[1]))\n",
    "print('Total Features: {}'.format(sampled_dataY.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "lca5-hjTopxL"
   },
   "outputs": [],
   "source": [
    "class GloGenAutoRegressive(Model_):\n",
    "    def __init__(self, enocder_hidden_state=200, decoder_hidden_state=200,\n",
    "                 output_diminsion=64, activation='relu', input_sequance_length=10,\n",
    "                 output_sequance_length=10, batch_size=100):\n",
    "      super(GloGenAutoRegressive, self).__init__()\n",
    "      self.batch_size = batch_size\n",
    "      self.output_diminsion = output_diminsion\n",
    "      self.encoder = LSTM(enocder_hidden_state, return_state=True, return_sequences=True)\n",
    "      self.decoder = LSTM(decoder_hidden_state, return_sequences=True, return_state=True)\n",
    "      self.dense_layer = TimeDistributed(Dense(output_diminsion, activation=activation)) \n",
    "      assert output_sequance_length % input_sequance_length == 0,f\"{output_sequance_length} is not divisible by {input_sequance_length}\"\n",
    "      self.output_sequance_length = output_sequance_length\n",
    "      self.input_sequance_length = input_sequance_length  \n",
    "\n",
    "    def call(self, inputs):\n",
    "      next_inputs = inputs\n",
    "      class_priors = inputs[:,:,-10:]\n",
    "      num_time_slices = int(self.output_sequance_length/self.input_sequance_length)\n",
    "      full_predicted_sequance = tf.zeros([self.batch_size,0,64])\n",
    "      #full_predicted_sequance = tf.reshape((), (0, num_time_slices))\n",
    "      print(full_predicted_sequance.shape)\n",
    "      #for loop for predicting the t*n sequance autoregressively from previously predicted sequance\n",
    "      for time_slice in range(num_time_slices) :\n",
    "        print(num_time_slices)\n",
    "        encoder_outputs, state_h, state_c = self.encoder(next_inputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "        output, _, _ = self.decoder(encoder_outputs, initial_state=encoder_states)\n",
    "        print(next_inputs.shape)\n",
    "        next_inputs = self.dense_layer(output)\n",
    "        print(next_inputs.shape)\n",
    "        #Appending the class priors to the next inputs\n",
    "        full_predicted_sequance = tf.concat((full_predicted_sequance, next_inputs), axis=1)\n",
    "        print(full_predicted_sequance.shape)\n",
    "        next_inputs = tf.concat((next_inputs, class_priors), axis=2)\n",
    "        print(next_inputs.shape)\n",
    "      return full_predicted_sequance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "s6YVjgYIJc9_"
   },
   "outputs": [],
   "source": [
    "class GloGen(Model_):\n",
    "    def __init__(self, enocder_hidden_state=200, decoder_hidden_state=200, output_diminsion=64, activation='relu'):\n",
    "        super(GloGen, self).__init__()\n",
    "        self.encoder = LSTM(enocder_hidden_state, return_state=True, return_sequences=True)\n",
    "        self.decoder = LSTM(decoder_hidden_state, return_sequences=True, return_state=True)\n",
    "        self.dense_layer = TimeDistributed(Dense(output_diminsion, activation=activation)) \n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_outputs, state_h, state_c = self.encoder(inputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "        output, _, _ = self.decoder(encoder_outputs, initial_state=encoder_states)\n",
    "        output = self.dense_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ocEBTNq1XTho"
   },
   "outputs": [],
   "source": [
    "class JointLoss() :\n",
    "  def __init__(self, lambda1=0.5, lambda2=0.5) :\n",
    "    self.lambda1 = lambda1\n",
    "    self.lambda2 = lambda2\n",
    "\n",
    "  def loss_joint(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "      diff_norm_2 = tf.math.reduce_sum(tf.square(tf.subtract(predicted_sequance_batch, target_sequance_batch)), axis=2)\n",
    "      return tf.reduce_sum(diff_norm_2, axis=1) \n",
    "\n",
    "  def loss_motion_flow(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "      predictions_tomporal_diffs = tf.experimental.numpy.diff(predicted_sequance_batch, axis=1)\n",
    "      real_tomporal_diffs = tf.experimental.numpy.diff(target_sequance_batch, axis=1)\n",
    "      prediction_motion_flow_diff_norm_2 = tf.reduce_sum(tf.square(tf.subtract(predictions_tomporal_diffs, real_tomporal_diffs)), axis=2)\n",
    "      return tf.reduce_sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
    "\n",
    "\n",
    "  def total_loss(self, target_sequance_batch, predicted_sequance_batch) :\n",
    "      joints_loss = self.loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
    "      motion_flow_loss = self.loss_motion_flow(predicted_sequance_batch, target_sequance_batch)\n",
    "      return self.lambda1*joints_loss + self.lambda2*motion_flow_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "IHiBCGcQ3vTw"
   },
   "outputs": [],
   "source": [
    "def run_experiment(learning_rate=0.002, lambda1=0.5, lambda2=0.5, use_mse=False\n",
    "                   , metrics=None, batch_size=100, epochs=50, validation_split=0.2) :\n",
    "  glogen_model = GloGenAutoRegressive(batch_size=batch_size, output_sequance_length=20)\n",
    "  if use_mse :\n",
    "    loss_function = tf.keras.losses.mean_squared_error\n",
    "  else :\n",
    "    loss_function = JointLoss(lambda1=lambda1, lambda2=lambda2).total_loss\n",
    "  glogen_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                       loss=loss_function, metrics=metrics)\n",
    "  history = glogen_model.fit(sampled_dataX, sampled_dataY,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs)\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EwF6affW5tNn",
    "outputId": "8beb2794-1d5a-423c-e16b-e4249190e670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "(100, 0, 64)\n",
      "2\n",
      "(100, 10, 74)\n",
      "(100, 10, 64)\n",
      "(100, 10, 64)\n",
      "(100, 10, 74)\n",
      "2\n",
      "(100, 10, 74)\n",
      "(100, 10, 64)\n",
      "(100, 20, 64)\n",
      "(100, 10, 74)\n",
      "(100, 0, 64)\n",
      "2\n",
      "(100, 10, 74)\n",
      "(100, 10, 64)\n",
      "(100, 10, 64)\n",
      "(100, 10, 74)\n",
      "2\n",
      "(100, 10, 74)\n",
      "(100, 10, 64)\n",
      "(100, 20, 64)\n",
      "(100, 10, 74)\n",
      "513/513 [==============================] - 126s 233ms/step - loss: 167247600.0000 - mean_absolute_percentage_error: 78.4862\n",
      "Epoch 2/10\n",
      "513/513 [==============================] - 119s 232ms/step - loss: 69175584.0000 - mean_absolute_percentage_error: 45.0501\n",
      "Epoch 3/10\n",
      "513/513 [==============================] - 119s 233ms/step - loss: 29720390.0000 - mean_absolute_percentage_error: 27.2025\n",
      "Epoch 4/10\n",
      "513/513 [==============================] - 119s 231ms/step - loss: 17268766.0000 - mean_absolute_percentage_error: 21.0369\n",
      "Epoch 5/10\n",
      "513/513 [==============================] - 119s 232ms/step - loss: 14466331.0000 - mean_absolute_percentage_error: 19.9057\n",
      "Epoch 6/10\n",
      "513/513 [==============================] - 122s 237ms/step - loss: 14071973.0000 - mean_absolute_percentage_error: 19.9589\n",
      "Epoch 7/10\n",
      "513/513 [==============================] - 117s 229ms/step - loss: 12781227.0000 - mean_absolute_percentage_error: 19.3671\n",
      "Epoch 8/10\n",
      "513/513 [==============================] - 118s 231ms/step - loss: 11804933.0000 - mean_absolute_percentage_error: 18.7730\n",
      "Epoch 9/10\n",
      "513/513 [==============================] - 118s 230ms/step - loss: 11789032.0000 - mean_absolute_percentage_error: 18.7902\n",
      "Epoch 10/10\n",
      "513/513 [==============================] - 117s 229ms/step - loss: 11786265.0000 - mean_absolute_percentage_error: 18.7898\n"
     ]
    }
   ],
   "source": [
    "history = run_experiment(epochs=10, lambda1=0.8, lambda2=0.2, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M-UuKxWD_Dyq",
    "outputId": "9a831596-b22c-432d-9e8f-cac17e13ede3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "618/618 [==============================] - 10s 9ms/step - loss: 149787.0781 - mean_absolute_percentage_error: 73.6768 - val_loss: 81618.0391 - val_mean_absolute_percentage_error: 50.4059\n",
      "Epoch 2/10\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 48777.3008 - mean_absolute_percentage_error: 36.0309 - val_loss: 26372.2559 - val_mean_absolute_percentage_error: 24.9939\n",
      "Epoch 3/10\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 17489.2148 - mean_absolute_percentage_error: 20.8270 - val_loss: 12661.5947 - val_mean_absolute_percentage_error: 18.4338\n",
      "Epoch 4/10\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 10844.3564 - mean_absolute_percentage_error: 17.6572 - val_loss: 10639.1816 - val_mean_absolute_percentage_error: 18.1014\n",
      "Epoch 5/10\n",
      "618/618 [==============================] - 5s 7ms/step - loss: 10035.3301 - mean_absolute_percentage_error: 17.5600 - val_loss: 10522.4854 - val_mean_absolute_percentage_error: 18.3110\n",
      "Epoch 6/10\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 9987.9561 - mean_absolute_percentage_error: 17.6414 - val_loss: 10521.6328 - val_mean_absolute_percentage_error: 18.3585\n",
      "Epoch 7/10\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 9983.4180 - mean_absolute_percentage_error: 17.6567 - val_loss: 10517.6064 - val_mean_absolute_percentage_error: 18.3350\n",
      "Epoch 8/10\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 9979.2148 - mean_absolute_percentage_error: 17.6467 - val_loss: 10514.8086 - val_mean_absolute_percentage_error: 18.3664\n",
      "Epoch 9/10\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 9973.8818 - mean_absolute_percentage_error: 17.6435 - val_loss: 10508.4971 - val_mean_absolute_percentage_error: 18.3418\n",
      "Epoch 10/10\n",
      "618/618 [==============================] - 5s 8ms/step - loss: 9966.6484 - mean_absolute_percentage_error: 17.6301 - val_loss: 10507.2559 - val_mean_absolute_percentage_error: 18.4288\n"
     ]
    }
   ],
   "source": [
    "#Trying MSE loss instead\n",
    "history = run_experiment(epochs=10, lambda1=0.8, lambda2=0.2, use_mse=True,\n",
    "                         metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWhIkQrmoT3j"
   },
   "source": [
    "Whole Glocal Net reimplementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJSiRvjwoXoE"
   },
   "outputs": [],
   "source": [
    "class"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
