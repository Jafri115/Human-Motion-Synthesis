{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QImRR28hJtIK",
    "outputId": "eb79c6bd-9d97-4081-b96a-563935bebd0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import Model as Model_\n",
    "from tensorflow.keras.layers import Input, PReLU, LeakyReLU, MaxPooling2D, Dropout, concatenate, UpSampling2D, ReLU, Conv2D, Flatten, Reshape, Conv1D, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Dense, BatchNormalization\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bWvUsyDtIAWu",
    "outputId": "2ee99dd8-a1a8-4127-b048-cd0a647e14b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwIsqTgBHvkG"
   },
   "source": [
    "# Loading and preprocessing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oqlP7j1UHvkG"
   },
   "outputs": [],
   "source": [
    "#Function to downsample the dataset to half of its size\n",
    "def downsample_dataset(dataset) :\n",
    "    return np.delete(dataset, np.s_[::2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rgfhc4kTmO8Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "class Dataset_Preprocessing:\n",
    "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, total_classes = 17):\n",
    "        \n",
    "        #Dataset Directory path\n",
    "        self.dir_path = dir_path\n",
    "        \n",
    "        #Which Dimension file to include, possible values: 2 and 3\n",
    "        self.include_dimension = include_dimension\n",
    "        \n",
    "        #Total frames in one Sample\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        #Activity classes to include\n",
    "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
    "        \n",
    "        #Total activity classes\n",
    "        self.total_classes = len(self.classes)\n",
    "        \n",
    "        #Subject Folders names in the Dataset\n",
    "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
    "    \n",
    "    def read_dataset(self):\n",
    "        try:\n",
    "            #Contains all the different activity vectors\n",
    "            activity_vector = {}\n",
    "            \n",
    "            #Contains the overall dataset\n",
    "            sampled_data = None\n",
    "            \n",
    "            #Based on dimensions, which folder to use for extracting the dataset files\n",
    "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
    "            \n",
    "            #Checking if the dataset path is valid\n",
    "            if not os.path.exists(self.dir_path):\n",
    "                print('The Data Directory Does not Exist!')\n",
    "                return None\n",
    "\n",
    "            #Iterating over all the subject folders\n",
    "            for fld in self.internal_folders:\n",
    "                #Iterating for each file in the specified folder\n",
    "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
    "                    #Extracting the activity from the filename\n",
    "                    activity = self.__extract_activity(file)\n",
    "                    \n",
    "                    if activity not in self.classes:\n",
    "                        continue\n",
    "                    \n",
    "                    #Reading the CSV file using Pandas\n",
    "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
    "\n",
    "                    #Formulating the activity vector using one hot encoding\n",
    "                    if activity not in activity_vector:\n",
    "                        total_keys = len(activity_vector.keys())\n",
    "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
    "                        activity_vector[activity][total_keys] = 1\n",
    "                    vector = activity_vector[activity]\n",
    "                    \n",
    "                    #Sampling the dataset\n",
    "                    grouped_sample = self.__group_samples(data, self.sample_size, vector)\n",
    "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
    "                    \n",
    "            return sampled_data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __extract_activity(self, filename):\n",
    "        try:\n",
    "            #Extracting the filename and excluding the extension\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            \n",
    "            #Substituting the empty string with characters other than english alphabets\n",
    "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
    "            return activity\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __group_samples(self, dataset, sample_size, activity):\n",
    "        try:\n",
    "            #Checking if the dataset is a Pandas Dataframe\n",
    "            if not isinstance(dataset, pd.DataFrame):\n",
    "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
    "                return None\n",
    "            \n",
    "            #Appending activity class to each row in the dataset\n",
    "            dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
    "            \n",
    "            #Reshaping the dataset into sample batches\n",
    "            total_samples = dataset.shape[0]//sample_size\n",
    "            total_features = dataset.shape[1]\n",
    "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
    "            \n",
    "            return grouped_rows\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "n3nQyeFUmO8b"
   },
   "outputs": [],
   "source": [
    "#For short term prediction, we need a sample size of 20(10 frames input sequance, 10 frames predicted sequance)\n",
    "sampled_data = Dataset_Preprocessing('/content/drive/MyDrive/Colab Notebooks/H3.6csv', sample_size=20).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nWBsWHhUHvkI",
    "outputId": "1b66c5bd-2c99-45cc-d2fd-d70a8a70ba7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77144, 20, 74)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKXdJCPKHvkI",
    "outputId": "bf454d85-9e08-4b8b-9c0b-6a97db9f6caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 77144\n",
      "Total Frames: 20\n",
      "Total Features: 74\n"
     ]
    }
   ],
   "source": [
    "print('Total Samples: {}'.format(sampled_data.shape[0]))\n",
    "print('Total Frames: {}'.format(sampled_data.shape[1]))\n",
    "print('Total Features: {}'.format(sampled_data.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LxUp2SBOHvkJ"
   },
   "outputs": [],
   "source": [
    "#Split the data into training, validation and test \n",
    "def split_data(data_array, validation_size=0.1, test_size=0.1) :\n",
    "    assert validation_size+test_size < 0.5 , f\"total size of validation and testing set should not exceed half of the dataset\"\n",
    "    assert validation_size > 0, f\"validation size should be greater than zero\"\n",
    "    assert test_size > 0, f\"test size should be greater than zero\"\n",
    "    \n",
    "    validation_step = int(1/validation_size)\n",
    "    test_step = int(1/test_size)\n",
    "    \n",
    "    \n",
    "    mask = np.ones(data_array.shape, dtype=bool)\n",
    "    validation_data = data_array[np.s_[::validation_step], :, :]\n",
    "    mask[np.s_[1::validation_step],:,:] = False\n",
    "    test_data = data_array[np.s_[1::test_step], :, :]\n",
    "    mask[np.s_[::test_step],:,:] = False\n",
    "    training_data = data_array[mask]\n",
    "    training_data_size = data_array.shape[0] - (validation_data.shape[0]+test_data.shape[0])\n",
    "    training_data = training_data.reshape((training_data_size, data_array.shape[1], data_array.shape[2]))\n",
    "    return training_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5ka0em25HvkJ"
   },
   "outputs": [],
   "source": [
    "def split_to_features_labels(dataset, input_sequance_size=10) :\n",
    "    assert input_sequance_size < dataset.shape[1], f\"input sequance should be smaller than the total sample size\"\n",
    "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
    "    labels = dataset[:,np.s_[input_sequance_size:], :]\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Wu2d2tB1HvkJ"
   },
   "outputs": [],
   "source": [
    "#Default parameters make up for 80%-10%-10% split\n",
    "training_data, validation_data, test_data = split_data(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXPODQ2uHvkJ",
    "outputId": "c592d4ac-e60d-464e-e489-89ec77cadfb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape is  (61714, 20, 74)\n",
      "Validation data shape is  (7715, 20, 74)\n",
      "Test data shape is  (7715, 20, 74)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape is \", training_data.shape)\n",
    "print(\"Validation data shape is \", validation_data.shape)\n",
    "print(\"Test data shape is \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "NCvYvSbdHvkK"
   },
   "outputs": [],
   "source": [
    "training_dataX, training_dataY = split_to_features_labels(training_data, input_sequance_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kF0Zdy5zHvkK",
    "outputId": "aade9773-2d50-49c0-8146-2173ea6a3e7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61714, 10, 74)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iW84ZLpPHvkK",
    "outputId": "0295d15e-0610-496d-ce01-dcc1b1bda528"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61714, 10, 74)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "O4RZmb88HvkL"
   },
   "outputs": [],
   "source": [
    "validation_dataX, validation_dataY = split_to_features_labels(validation_data, input_sequance_size=10)\n",
    "test_dataX, test_dataY = split_to_features_labels(test_data, input_sequance_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXz60WWaHvkL"
   },
   "source": [
    "# Start of model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM391-I5HvkL"
   },
   "source": [
    "## Short term model implementation (Glogen only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "KAwzUZ8KLdJS"
   },
   "outputs": [],
   "source": [
    "class GloGen_Bidirectional_RNN_encoder(Model_):\n",
    "    def __init__(self, num_recurrent_neurons=200):\n",
    "        super(GloGen_Bidirectional_RNN_encoder, self).__init__()\n",
    "\n",
    "        #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "        self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "    def call(self, input_x):\n",
    "        output = self.recurrent_layer(input_x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "TqFfWtIpSXIC"
   },
   "outputs": [],
   "source": [
    "class GloGen_Bidirectional_RNN_decoder(Model_):\n",
    "  def __init__(self, num_recurrent_neurons=64):\n",
    "    super(GloGen_Bidirectional_RNN_decoder, self).__init__()\n",
    "    #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "    self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "  def call(self, encoder_output):\n",
    "    glogen_output = self.recurrent_layer(encoder_output)\n",
    "    #output = tf.one_hot(tf.argmax(output, axis = 1), depth=3)\n",
    "    return glogen_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "drWI_7iPHvkM"
   },
   "outputs": [],
   "source": [
    "class GloGen(Model_):\n",
    "    def __init__(self, embedding_diminisions=200, decoder_output_diminsions=64):\n",
    "        super(GloGen, self).__init__()\n",
    "        self.encoder = GloGen_Bidirectional_RNN_encoder(embedding_diminisions)\n",
    "        self.decoder = GloGen_Bidirectional_RNN_decoder(decoder_output_diminsions)\n",
    "\n",
    "    def call(self, input_sequance):\n",
    "        encoder_states = self.encoder(input_sequance)\n",
    "        glogen_output = self.decoder(encoder_states)\n",
    "        return glogen_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "QizN2JrjaobT"
   },
   "outputs": [],
   "source": [
    "glogen_bidirectional_RNN_encoder = GloGen_Bidirectional_RNN_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MNaOr-LGauZP"
   },
   "outputs": [],
   "source": [
    "#simulating on validation data as training data is too larget to run on one go\n",
    "output_encoder = glogen_bidirectional_RNN_encoder(validation_dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqNyaMUPa8LA",
    "outputId": "a072a0e6-e301-4846-87aa-76413d4a3bd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([7715, 10, 200])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of 10x200 predictions for each sample inside the batch\n",
    "output_encoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "B7kOinVXfTKt"
   },
   "outputs": [],
   "source": [
    "#Simulating the output shape for the decoder\n",
    "glogen_decoder = GloGen_Bidirectional_RNN_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "oLxF5BCB9pTX"
   },
   "outputs": [],
   "source": [
    "glogen_sparse_output_predictions = glogen_decoder(output_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQT9RhgJ90rB",
    "outputId": "7e881993-9b1b-4f70-93e7-df29c5ddbd8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([7715, 10, 64])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction for 32 joints position for the next 10 timesteps for each sample inside the batch\n",
    "glogen_sparse_output_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WKmRas0RHvkN"
   },
   "outputs": [],
   "source": [
    "glogen_model= GloGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ZYi8G8_bHvkO"
   },
   "outputs": [],
   "source": [
    "glogen_sparse_output_predictions = glogen_model(validation_dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTlwFHgnHvkO",
    "outputId": "245e0614-f48f-44f8-8d4a-cabf2d09450c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([7715, 10, 64])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glogen_sparse_output_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "ggz_bg8soyjJ"
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model, mb = 8, lr = 0.002, opt=tf.keras.optimizers.Adam, lambda_1=0.5, lambda_2=0.5):\n",
    "        self.model     = model\n",
    "        self.optimizer = opt(learning_rate = lr)\n",
    "        self.mb        = mb\n",
    "\n",
    "        self.lambda_1 = lambda_1\n",
    "        self.lambda_2 = lambda_2\n",
    "\n",
    "        self.train_loss     = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.test_loss     = tf.keras.metrics.Mean(name='test_loss')\n",
    "  \n",
    "    @tf.function\n",
    "    def train_step(self, x , y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x)\n",
    "            #TODO Get the features in a proper way\n",
    "            loss = self.total_loss(y[:,:,:64], predictions)\n",
    "\n",
    "        #print(self.model.trainable_variables)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        self.train_loss(loss)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, x , y):\n",
    "        predictions = self.model(x)\n",
    "        #TODO Get the features in a proper way\n",
    "        loss = self.total_loss(y[:,:,:64], predictions)\n",
    "        self.test_loss(loss)\n",
    "        \n",
    "    def train(self):\n",
    "        for mbX, mbY in self.train_ds:\n",
    "            self.train_step(mbX, mbY)\n",
    "\n",
    "    def test(self):\n",
    "        for mbX, mbY in self.test_ds:\n",
    "            self.test_step(mbX, mbY)    \n",
    "\n",
    "    def run(self, dataX, dataY, testX, testY, epochs, verbose=2):\n",
    "        historyTR = []\n",
    "        historyTS = []\n",
    "        template = '{} {}, {}: {}, {}: {}'\n",
    "        self.train_ds = tf.data.Dataset.from_tensor_slices((dataX, dataY)).shuffle(16000).batch(self.mb)\n",
    "        self.test_ds  = tf.data.Dataset.from_tensor_slices((testX,testY)).batch(self.mb)\n",
    "        for i in range(epochs):\n",
    "\n",
    "            self.train ()\n",
    "            #   print(lossTR)\n",
    "            self.test  ()\n",
    "            if verbose > 0:\n",
    "                print(template.format(\"epoch: \", i+1,\n",
    "                              \" TRAIN LOSS: \", self.train_loss.result(),\n",
    "                              \" TEST LOSS: \" , self.test_loss.result()))\n",
    "                               #\" TRAIN ACC: \" , self.train_accuracy.result()*100,\n",
    "                               #\" TEST ACC: \"  , self.test_accuracy.result()*100) )\n",
    "\n",
    "            temp = '{}'\n",
    "            historyTR.append(float(temp.format(self.train_loss.result())))\n",
    "            historyTS.append(float(temp.format(self.test_loss.result() )))\n",
    "\n",
    "            self.train_loss.reset_states()\n",
    "            self.test_loss.reset_states()\n",
    "        return historyTR, historyTS\n",
    "\n",
    "    #Defining the loss function utilities\n",
    "    def loss_joint(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        diff_norm_2 = tf.math.reduce_sum(tf.square(tf.subtract(predicted_sequance_batch, target_sequance_batch)), axis=2)\n",
    "        return tf.reduce_sum(diff_norm_2, axis=1) \n",
    "\n",
    "    def loss_motion_flow(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        predictions_tomporal_diffs = tf.experimental.numpy.diff(predicted_sequance_batch, axis=1)\n",
    "        real_tomporal_diffs = tf.experimental.numpy.diff(target_sequance_batch, axis=1)\n",
    "        prediction_motion_flow_diff_norm_2 = tf.reduce_sum(tf.square(tf.subtract(predictions_tomporal_diffs, real_tomporal_diffs)), axis=2)\n",
    "        return tf.reduce_sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
    "    \n",
    "    \n",
    "    def total_loss(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        joints_loss = self.loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
    "        motion_flow_loss = self.loss_motion_flow(predicted_sequance_batch, target_sequance_batch)\n",
    "        return self.lambda_1*joints_loss + self.lambda_2*motion_flow_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "o9VC6SDaJvb0"
   },
   "outputs": [],
   "source": [
    "training_dataX = np.float32(training_dataX)\n",
    "training_dataY = np.float32(training_dataY)\n",
    "validation_dataX = np.float32(validation_dataX)\n",
    "validation_dataY = np.float32(validation_dataY)\n",
    "test_dataX = np.float32(test_dataX)\n",
    "test_dataY = np.float32(test_dataY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "XNcisnRXHvkO"
   },
   "outputs": [],
   "source": [
    "#Running experiment for short term prediction training \n",
    "glogen_model = GloGen()\n",
    "optimizer = Optimizer(glogen_model, mb=100, lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azy8sR9HHvkP",
    "outputId": "554da7f8-9fe8-4f3f-a67c-51b999c4a793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1,  TRAIN LOSS: : 77421632.0,  TEST LOSS: : 77381944.0\n",
      "epoch:  2,  TRAIN LOSS: : 77375424.0,  TEST LOSS: : 77351376.0\n",
      "epoch:  3,  TRAIN LOSS: : 77353496.0,  TEST LOSS: : 77336920.0\n",
      "epoch:  4,  TRAIN LOSS: : 77342992.0,  TEST LOSS: : 77329032.0\n",
      "epoch:  5,  TRAIN LOSS: : 77336920.0,  TEST LOSS: : 77324376.0\n",
      "epoch:  6,  TRAIN LOSS: : 77333144.0,  TEST LOSS: : 77321240.0\n",
      "epoch:  7,  TRAIN LOSS: : 77330424.0,  TEST LOSS: : 77318952.0\n",
      "epoch:  8,  TRAIN LOSS: : 77328376.0,  TEST LOSS: : 77317200.0\n",
      "epoch:  9,  TRAIN LOSS: : 77326888.0,  TEST LOSS: : 77315856.0\n",
      "epoch:  10,  TRAIN LOSS: : 77325648.0,  TEST LOSS: : 77314696.0\n"
     ]
    }
   ],
   "source": [
    "#Running with low learning rate\n",
    "training_loss_history, validation_loss_history = optimizer.run(training_dataX, training_dataY, validation_dataX, validation_dataY, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cn910-g-HvkP"
   },
   "source": [
    "## Long term prediction model (Adding interpolation and Locgen) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZG4LavnDCeyA"
   },
   "outputs": [],
   "source": [
    "#returns a list of the interpolated frames between x0 and x1\n",
    "def interpolateFrames(glogen_sparse_output_predictions, num_of_frames=5) :\n",
    "  batch_size = glogen_sparse_output_predictions.shape[0]\n",
    "  timesteps = glogen_sparse_output_predictions.shape[1]\n",
    "  features = glogen_sparse_output_predictions.shape[2]\n",
    "  interpolated_frames = np.zeros((batch_size, timesteps, num_of_frames, features))\n",
    "  for batch in range(glogen_sparse_output_predictions.shape[0]) :\n",
    "    for t in range(glogen_sparse_output_predictions.shape[1]-1) :\n",
    "      for j in range(num_of_frames) :\n",
    "        X_i0 = glogen_sparse_output_predictions[batch, t]\n",
    "        X_i1 = glogen_sparse_output_predictions[batch, t+1]\n",
    "        \n",
    "        alpha_j = j/num_of_frames\n",
    "        current_frame = alpha_j*X_i0 + (1-alpha_j)*X_i1\n",
    "        interpolated_frames[batch, t, j] = current_frame\n",
    "\n",
    "  return interpolated_frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80Iu58hSH-ms"
   },
   "outputs": [],
   "source": [
    "interpolated_frames = interpolateFrames(glogen_sparse_output_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DZXeYTXJWup",
    "outputId": "5084801d-4593-4ea9-b2ae-fea1aeb346de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 5, 64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of interpolated frames should be 10(batch size) x 10(timesteps)\n",
    "#x 5(num of interpolated frames between these timesteps) x 64(num_features)\n",
    "interpolated_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNiC2u_vMgHa"
   },
   "outputs": [],
   "source": [
    "#timesteps and interpolated frames diminsion can be flattened to 10(batch size) x 50(dense frames) x 64(features)\n",
    "interpolated_frames = np.reshape(interpolated_frames, (10, 50, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDsfIXJf93EL",
    "outputId": "77fd2bcf-2aa6-4fef-81f0-3a1d522ee316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer loc_gen__bidirectional_rnn_encoder_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "loc gen encoder output is  (10, 50, 200)\n",
      "final predictions shape is  (10, 50, 64)\n"
     ]
    }
   ],
   "source": [
    "#Simulating the locgen phase with batches as well\n",
    "locgen_encoder = LocGen_Bidirectional_RNN_encoder()\n",
    "locgen_encoder_output = locgen_encoder(interpolated_frames)\n",
    "print(\"loc gen encoder output is \", locgen_encoder_output.shape)\n",
    "\n",
    "locgen_decoder = LocGen_Bidirectional_RNN_decoder()\n",
    "final_predictions = locgen_decoder(locgen_encoder_output)\n",
    "\n",
    "print(\"final predictions shape is \", final_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQ3DZUkxLAQA"
   },
   "outputs": [],
   "source": [
    "class LocGen_Bidirectional_RNN_encoder(Model_) :\n",
    "  def __init__(self, num_recurrent_neurons=200) :\n",
    "    super(LocGen_Bidirectional_RNN_encoder, self).__init__()\n",
    "    #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "    self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "\n",
    "  def call(self, interpolated_output) :\n",
    "    predictions = self.recurrent_layer(interpolated_output)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZPdNS_Em6dX"
   },
   "outputs": [],
   "source": [
    "class LocGen_Bidirectional_RNN_decoder(Model_) :\n",
    "  def __init__(self, num_recurrent_neurons=64) :\n",
    "    super(LocGen_Bidirectional_RNN_decoder, self).__init__()\n",
    "    #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "    self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "  def call(self, encoder_output) :\n",
    "    final_predictions = self.recurrent_layer(encoder_output)\n",
    "    return final_predictions"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Glocal_Net_Reimplementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
